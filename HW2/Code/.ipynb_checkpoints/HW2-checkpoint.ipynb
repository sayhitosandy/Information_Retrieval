{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import STOPWORDS\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "from autocorrect import spell\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "stemmer = SnowballStemmer('english')\n",
    "stops = set(stopwords.words('english'))\n",
    "for i in stops:\n",
    "    STOPWORDS.add(i)\n",
    "# print(STOPWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_dic = {} #Inverted index with tfs\n",
    "count_to_file = {} #File No. to file path mapping\n",
    "file_to_count = {} #File path to file no. mapping\n",
    "doc_vector = {} #doc to term mapping with tf-idf (temp)\n",
    "count = 0 #Total no. of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "folder = os.path.join(cwd, r\"stories\")\n",
    "\n",
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        if (\".html\" in file or \".descs\" in file or \".header\" in file or \".footer\" in file or \".musings\" in file):\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(root, file)\n",
    "        count += 1\n",
    "\n",
    "        lines = \"\"\n",
    "\n",
    "        file_to_count[file] = count\n",
    "        count_to_file[count] = file\n",
    "\n",
    "        local_freq_dic = {} #store only the freq of words appearing in one file\n",
    "        \n",
    "        try:\n",
    "            f = open(file_path, 'r')\n",
    "            lines = f.read()\n",
    "\n",
    "        except Exception as e:\n",
    "            f = open(file_path, 'r', encoding='mbcs')  #read ansi\n",
    "            lines = f.read()\n",
    "\n",
    "        words = tokenizer.tokenize(lines) #tokenize\n",
    "        words = [word.strip('_ ').lower() for word in words if word not in STOPWORDS] #lowercase \n",
    "        words = [stemmer.stem(word) for word in words] #stemmer\n",
    "        words = [word for word in words if word not in STOPWORDS and len(word) > 0] #stopword removal\n",
    "        \n",
    "        for word in words:\n",
    "            if word not in local_freq_dic:\n",
    "                local_freq_dic[word] = 0\n",
    "            local_freq_dic[word] += 1\n",
    "            \n",
    "        for word in local_freq_dic:\n",
    "            if word not in tf_dic:\n",
    "                tf_dic[word] = []\n",
    "            tf_dic[word].append([count, local_freq_dic[word]])\n",
    "            \n",
    "        if count not in doc_vector:\n",
    "            doc_vector[count] = []\n",
    "        for word in local_freq_dic:\n",
    "            doc_vector[count].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for word in tf_dic:\n",
    "#     tf_dic[word] = sorted(tf_dic[word])\n",
    "\n",
    "# tf_dic['ow']\n",
    "# tf_dic['humus']\n",
    "# tf_dic['water']\n",
    "# for l in tf_dic['water']:\n",
    "#     print(l[0], l[1])\n",
    "#     print(tf)\n",
    "\n",
    "f = open(os.path.join(cwd, \"file_names.txt\"), 'r')\n",
    "\n",
    "while True:\n",
    "    \n",
    "    line1 = f.readline()\n",
    "    if (not line1):\n",
    "        break\n",
    "    \n",
    "    line2 = f.readline()\n",
    "    \n",
    "    file_name = line1.split()[0]\n",
    "    file_title = line2\n",
    "    \n",
    "#     for i in file_to_count.keys():\n",
    "#         if (file_name in i):\n",
    "# #             print(file_name)\n",
    "#             c = file_to_count[i]\n",
    "# #             print(i)\n",
    "#             break\n",
    "    c = file_to_count[file_name]\n",
    "    \n",
    "    words = tokenizer.tokenize(file_title) #tokenize\n",
    "    words = [word.strip('_ ').lower() for word in words if word not in STOPWORDS] #lowercase \n",
    "    words = [stemmer.stem(word) for word in words] #stemmer\n",
    "    words = [word for word in words if word not in STOPWORDS and len(word) > 0] #stopword removal\n",
    "#     print(words)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in tf_dic:\n",
    "            res = tf_dic[word]\n",
    "#             print(res)\n",
    "            for doc_tf in tf_dic[word]:\n",
    "                if doc_tf[0] == c:\n",
    "                    \n",
    "                    for i in res:\n",
    "                        if (i == doc_tf):\n",
    "                            res.remove(i)\n",
    "                    \n",
    "                    doc_tf[1] += 5 #special attention to title terms\n",
    "                    res.append(doc_tf)\n",
    "            tf_dic[word] = res\n",
    "#             print(tf_dic[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idf_dic = {} #idf(t)\n",
    "wt_dic = {} #inverted index with tf-idf [term to doc]\n",
    "doc_dic = {} #reversed inverted index with tf-idf [doc to term]\n",
    "\n",
    "for word in tf_dic:\n",
    "    idf_dic[word] = np.log10((count+1)/len(tf_dic[word]))\n",
    "    \n",
    "for word in tf_dic:\n",
    "    wt_dic[word] = []\n",
    "    idf = idf_dic[word]\n",
    "    \n",
    "    for doc_tf in tf_dic[word]:\n",
    "        wt_dic[word].append([doc_tf[0], (1 + np.log10(doc_tf[1])) * idf])\n",
    "\n",
    "for doc_id in doc_vector:\n",
    "    doc_dic[doc_id] = []\n",
    "    \n",
    "    for word in doc_vector[doc_id]:\n",
    "        for res in wt_dic[word]:\n",
    "            if (res[0] == doc_id):\n",
    "                doc_dic[doc_id].append([word, res[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(tf_dic['water'])\n",
    "# print()\n",
    "# print(idf_dic['water'])\n",
    "# print()\n",
    "# print(wt_dic['water'])\n",
    "# print()\n",
    "# print(doc_dic[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp = open(os.path.join(cwd, r\"tf_dic.json\"), 'w+')\n",
    "json.dump(tf_dic, fp, sort_keys=True)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r\"count_to_file.json\"), 'w+')\n",
    "json.dump(count_to_file, fp, sort_keys=True)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r\"file_to_count.json\"), 'w+')\n",
    "json.dump(file_to_count, fp, sort_keys=True)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r\"idf_dic.json\"), 'w+')\n",
    "json.dump(idf_dic, fp, sort_keys=True)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r\"wt_dic.json\"), 'w+')\n",
    "json.dump(wt_dic, fp, sort_keys=True)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r\"doc_dic.json\"), 'w+')\n",
    "json.dump(doc_dic, fp, sort_keys=True)\n",
    "fp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # %matplotlib inline\n",
    "# import matplotlib.pyplot as plt\n",
    "# from wordcloud import WordCloud, STOPWORDS\n",
    "# wordcloud = WordCloud(stopwords=STOPWORDS, background_color='white',ranks_only=True).generate_from_frequencies(idf_dic)\n",
    "# # plt.figure(figsize=(15,15))\n",
    "# plt.figure()\n",
    "# plt.imshow(wordcloud, interpolation='bilinear')\n",
    "# plt.axis('off')\n",
    "# plt.show()\n",
    "\n",
    "# wordcloud.to_file(os.path.join(cwd, \"cloud.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32318\n",
      "32318\n",
      "32318\n",
      "467\n",
      "467\n",
      "467\n",
      "467\n"
     ]
    }
   ],
   "source": [
    "print(len(tf_dic))\n",
    "print(len(idf_dic))\n",
    "print(len(wt_dic))\n",
    "print(len(doc_dic))\n",
    "print(len(count_to_file))\n",
    "print(len(file_to_count))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fp = open(os.path.join(cwd, r\"wt_dic.json\"), 'r')\n",
    "wt_dic = json.load(fp)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r\"count_to_file.json\"), 'r')\n",
    "count_to_file = json.load(fp)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r\"file_to_count.json\"), 'r')\n",
    "file_to_count = json.load(fp)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r\"doc_dic.json\"), 'r')\n",
    "doc_dic = json.load(fp)\n",
    "fp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_cache = []\n",
    "query_ans_cache = []\n",
    "query_type = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose one among the following:\n",
      "        1. Tf-idf based\n",
      "        2. Cosine similarity based\n",
      "    Enter Option: 1\n",
      "Enter query: 50,000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# while (True):\n",
    "print('''Choose one among the following:\n",
    "        1. Tf-idf based\n",
    "        2. Cosine similarity based\n",
    "    Enter Option: ''', end='')\n",
    "option = input()\n",
    "\n",
    "print(\"Enter query: \", end='')\n",
    "query = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['of', '000']\n",
      "['000']\n"
     ]
    }
   ],
   "source": [
    "query_terms = tokenizer.tokenize(query) #tokenize\n",
    "query_terms = [word.strip('_ ').lower() for word in query_terms if word not in STOPWORDS] #lowercase \n",
    "query_terms = [spell(word) for word in query_terms] #spell checker\n",
    "query_terms = [stemmer.stem(word) for word in query_terms] #stemmer\n",
    "query_terms = [word for word in query_terms if word not in STOPWORDS and len(word) > 0] #stopword removal\n",
    "\n",
    "print(query_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print(type(option))\n",
    "\n",
    "final_ans = []\n",
    "\n",
    "flag = 0\n",
    "for i in range(len(query_cache)):\n",
    "    if (query_type[i] == option and sorted(query_cache[i]) == sorted(query_terms)):\n",
    "        for j in query_ans_cache[i]:\n",
    "            print(j)\n",
    "        flag = 1\n",
    "\n",
    "# print(len(query_cache))\n",
    "\n",
    "if (flag == 0):\n",
    "    if (len(query_cache) == 20):\n",
    "        del query_cache[0]\n",
    "        del query_ans_cache[0]\n",
    "        del query_type[0]\n",
    "    query_cache.append(query_terms)\n",
    "    query_type.append(option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (flag == 0 and option == '1'):\n",
    "    relevant_docs = {}\n",
    "\n",
    "    for word in query_terms:\n",
    "    #     print(word, wt_dic[word])\n",
    "        if word in wt_dic:\n",
    "            for doc_tfidf in wt_dic[word]:\n",
    "                doc_id = doc_tfidf[0]\n",
    "                wt = doc_tfidf[1]\n",
    "\n",
    "                if (doc_id not in relevant_docs):\n",
    "                    relevant_docs[doc_id] = 0\n",
    "                relevant_docs[doc_id] += wt\n",
    "\n",
    "    # print(relevant_docs)\n",
    "    if len(relevant_docs) > 0:\n",
    "        sorted_docs = sorted(relevant_docs, key=relevant_docs.get, reverse=True)\n",
    "        for doc in sorted_docs[:5]:\n",
    "#             print(count_to_file[str(doc)])\n",
    "            final_ans.append(count_to_file[str(doc)])\n",
    "\n",
    "    else:\n",
    "#         print(\"No document found\")\n",
    "        final_ans.append(\"No document found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if (flag == 0 and option == '2'):\n",
    "    tf_q = {} #tf dict for query words\n",
    "    idf_q = {} #idf dict for query words\n",
    "    wt_q = {} #tf-idf dict for query words\n",
    "    \n",
    "    local_freq_dic = {}\n",
    "    \n",
    "    for term in query_terms:\n",
    "        if term not in local_freq_dic:\n",
    "            local_freq_dic[term] = 0\n",
    "        local_freq_dic[term] += 1\n",
    "    \n",
    "    for term in local_freq_dic:\n",
    "        if term not in tf_q:\n",
    "            tf_q[term] = []\n",
    "        tf_q[term].append(local_freq_dic[term])\n",
    "        \n",
    "    for term in tf_q:\n",
    "        idf_q[term] = np.log10((count+1)/len(tf_q[term]))\n",
    "\n",
    "    for term in tf_q:\n",
    "        wt_q[term] = []\n",
    "        idf = idf_q[term]\n",
    "\n",
    "        for tf in tf_q[term]:\n",
    "            wt_q[term].append((1 + np.log10(tf)) * idf)\n",
    "            \n",
    "#     print(wt_q)\n",
    "    \n",
    "    docs = set() #set of all relevant docs (union)\n",
    "    for term in wt_q:\n",
    "        if (term in wt_dic):\n",
    "            res = wt_dic[term]\n",
    "            for pair in res:\n",
    "                docs.add(pair[0])\n",
    "    \n",
    "    cos_score = {} #cosine score for each relevant doc\n",
    "#     print(docs)\n",
    "    \n",
    "    for doc_id in docs:\n",
    "        #find_cosine_sim(q, doc_id)\n",
    "        \n",
    "        doc_id = str(doc_id)\n",
    "        cos_score[doc_id] = 0\n",
    "        \n",
    "        s = 0 #q.d\n",
    "#         doc_term = set()\n",
    "\n",
    "        for term in wt_q:\n",
    "            for pair in doc_dic[doc_id]:\n",
    "                if (pair[0] == term):\n",
    "                    s += (wt_q[term][0]*pair[1]) #compute q.d\n",
    "#                     doc_term.add(pair[1])\n",
    "\n",
    "        s1 = 0                \n",
    "#         doc_terms = list(doc_term)\n",
    "#         for i in doc_terms:\n",
    "#             s1 += (i*i) #compute d^2\n",
    "        doc_terms = doc_dic[doc_id]\n",
    "        for i in doc_terms:\n",
    "            s1 += (i[1]*i[1]) #compute d^2\n",
    "\n",
    "        s2 = 0\n",
    "        for term in wt_q:\n",
    "            s2 += (wt_q[term][0]*wt_q[term][0]) #compute q^2\n",
    "\n",
    "        s1 = math.sqrt(s1) #compute |d|\n",
    "        s2 = math.sqrt(s2) #compute |q|\n",
    "\n",
    "        if (s1 == 0 or s2 == 0):\n",
    "            s = 0\n",
    "        else:\n",
    "            s = s/(s1*s2) #sim = q.d/|q||d|\n",
    "    \n",
    "        cos_score[doc_id] = s\n",
    "        \n",
    "    relevant_docs = sorted(cos_score, key=cos_score.get, reverse=True)\n",
    "    if (cos_score[relevant_docs[0]] > 0):\n",
    "        for doc in relevant_docs[:5]:\n",
    "#             print(doc, count_to_file[str(doc)], cos_score[doc])\n",
    "            final_ans.append(count_to_file[str(doc)])\n",
    "    else:\n",
    "#         print(\"No documents found\")\n",
    "        final_ans.append(\"No document found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dakota.txt', 'cardcnt.txt', 'keepmodu.txt', 'blabnove.txt', 'rocket.sf']\n"
     ]
    }
   ],
   "source": [
    "if (flag == 0):\n",
    "    query_ans_cache.append(final_ans)\n",
    "    print(final_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: num2words in c:\\users\\sanidhya\\anaconda3\\lib\\site-packages\n"
     ]
    }
   ],
   "source": [
    "!pip install num2words"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
