{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import packages\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import STOPWORDS\n",
    "from random import shuffle\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Pre-processing\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+') #Tokenizer\n",
    "stemmer = SnowballStemmer('english') #Snowball Stemmer\n",
    "stops = set(stopwords.words('english')) #Stopwords\n",
    "for i in stops:\n",
    "    STOPWORDS.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd() #Current Working Directory\n",
    "folders_path = os.path.join(cwd, r\"Dataset\\20_newsgroups\")\n",
    "folders = os.listdir(folders_path) #List of folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_to_file = {} #Dictionary that maps file no to file path\n",
    "file_to_count = {} #Dictionary that maps file path to file no\n",
    "count = 0\n",
    "\n",
    "for root, _, files in os.walk(folders_path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        count += 1\n",
    "        \n",
    "        file_to_count[file_path] = str(count)\n",
    "        count_to_file[str(count)] = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = {} #Dictionary that stores list of training files for each class\n",
    "test_data = {} #Dictionary that stores list of test files for each class\n",
    "class_dic = {} #Dictionary that maps class no to class/folder path\n",
    "cls_count = 0\n",
    "ratio = 0.7 #Training Data split ratio\n",
    "\n",
    "for root, _, files in os.walk(folders_path):\n",
    "    if (len(files) == 0):\n",
    "        continue\n",
    "    \n",
    "    class_dic[str(cls_count)] = root\n",
    "    train_data_len = int(ratio * len(files))\n",
    "    test_data_len = len(files) - train_data_len\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        data.append(file_to_count[file_path])\n",
    "    \n",
    "    shuffle(data) #Randomly shuffle data to split into test and train\n",
    "    \n",
    "    training_data[str(cls_count)] = data[:train_data_len]\n",
    "    test_data[str(cls_count)] = data[-test_data_len:]\n",
    "    \n",
    "    cls_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15514\n",
      "12816\n",
      "14963\n",
      "14904\n",
      "16221\n"
     ]
    }
   ],
   "source": [
    "word_dic = {} #Dictionary that stores vocabulary for each class\n",
    "freq_dic = {} #Dictionary that stores the frequency of occurance of each word in the vocabulary for each class\n",
    "\n",
    "for cls in class_dic:\n",
    "    \n",
    "    if cls not in word_dic:\n",
    "        word_dic[cls] = {}\n",
    "        freq_dic[cls] = {}\n",
    "    \n",
    "    files = training_data[cls]\n",
    "    for file in files:\n",
    "        \n",
    "        file_path = count_to_file[file]\n",
    "    \n",
    "        lines = \"\"\n",
    "        \n",
    "        f = open(file_path, 'r')\n",
    "        lines = f.read()\n",
    "        \n",
    "        words = tokenizer.tokenize(lines) #tokenize\n",
    "        words = [word.strip('_ ').lower() for word in words if word not in STOPWORDS] #lowercase \n",
    "        words = [stemmer.stem(word) for word in words] #stemmer\n",
    "        words = [word for word in words if word not in STOPWORDS and len(word) > 0] #stopword removal\n",
    "\n",
    "        for word in words:\n",
    "            if word not in word_dic[cls]:\n",
    "                word_dic[cls][word] = set()\n",
    "                freq_dic[cls][word] = 0\n",
    "            word_dic[cls][word].add(file)\n",
    "            freq_dic[cls][word] += 1\n",
    "    \n",
    "    print(len(word_dic[cls]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cls in word_dic:\n",
    "    for i in word_dic[cls]:\n",
    "        word_dic[cls][i] = sorted(word_dic[cls][i]) #Sort the docIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47154\n"
     ]
    }
   ],
   "source": [
    "word_dic_complete = set() #Find the no. of words in the complete vocabulary (For smoothing denominator)\n",
    "\n",
    "for cls in class_dic:\n",
    "    for word in word_dic[cls].keys():\n",
    "            word_dic_complete.add(word)\n",
    "            \n",
    "total_vocab_size = len(word_dic_complete)\n",
    "print(total_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store dictionaries as json files\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\count_to_file.json'), 'w+')\n",
    "json.dump(count_to_file, fp, sort_keys=True)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\file_to_count.json'), 'w+')\n",
    "json.dump(file_to_count, fp, sort_keys=True)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\training_data.json'), 'w+')\n",
    "json.dump(training_data, fp, sort_keys=True)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\test_data.json'), 'w+')\n",
    "json.dump(test_data, fp, sort_keys=True)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\class_dic.json'), 'w+')\n",
    "json.dump(class_dic, fp, sort_keys=True)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\word_dic.json'), 'w+')\n",
    "json.dump(word_dic, fp, sort_keys=True)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\freq_dic.json'), 'w+')\n",
    "json.dump(freq_dic, fp, sort_keys=True)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load dictionaries\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\count_to_file.json'), 'r')\n",
    "count_to_file = json.load(fp)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\file_to_count.json'), 'r')\n",
    "file_to_count = json.load(fp)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\training_data.json'), 'r')\n",
    "training_dataa = json.load(fp)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\test_data.json'), 'r')\n",
    "test_data = json.load(fp)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\class_dic.json'), 'r')\n",
    "class_dic = json.load(fp)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\word_dic.json'), 'r')\n",
    "word_dic = json.load(fp)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\freq_dic.json'), 'r')\n",
    "freq_dic = json.load(fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## NB Training\n",
    "\n",
    "prior = {} #Dictionary that stores log of prior probability for each class \n",
    "cond_prob = {} #Dictionary that stores log of conditional probability for each token in the training data for each class\n",
    "\n",
    "N = 0 #Total no. of training docs\n",
    "for cls in class_dic:\n",
    "    N += len(training_data[cls])\n",
    "    \n",
    "for cls in class_dic:\n",
    "    Nc = len(training_data[cls]) #No. of docs belonging to class cls\n",
    "    \n",
    "    # Calculate prior prob\n",
    "    if cls not in prior:\n",
    "        prior[cls] = np.log(Nc/N) #log_e\n",
    "    \n",
    "    #Calculate conditonal prob\n",
    "    if cls not in cond_prob:\n",
    "        cond_prob[cls] = {}\n",
    "    \n",
    "    summ = 0\n",
    "    for t_ in freq_dic[cls]:\n",
    "        summ += freq_dic[cls][t_] \n",
    "    summ += total_vocab_size #Smoothing (Denominator)\n",
    "        \n",
    "    for t in word_dic[cls]:\n",
    "        tmp = freq_dic[cls][t] + 1 #Smoothing (Numerator)\n",
    "        cond_prob[cls][t] = np.log(tmp/summ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save prior, conditional prob as json file \n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\prior.json'), 'w+')\n",
    "json.dump(prior, fp, sort_keys=True)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\cond_prob.json'), 'w+')\n",
    "json.dump(cond_prob, fp, sort_keys=True)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load prior, conditional prob\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\prior.json'), 'r')\n",
    "prior = json.load(fp)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\cond_prob.json'), 'r')\n",
    "cond_prob = json.load(fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## NB Testing\n",
    "\n",
    "acc_count = 0 #Count for correct classification\n",
    "\n",
    "N_test = 0 #Total no. of test docs\n",
    "for cls in class_dic:\n",
    "    N_test += len(test_data[cls])\n",
    "\n",
    "class_true = [] #List of true classes for each doc in test set\n",
    "class_found = [] #List of predicted classes for each doc in test set\n",
    "    \n",
    "for cls in class_dic:\n",
    "    test_docs = test_data[cls]\n",
    "    \n",
    "    for doc in test_docs:\n",
    "        \n",
    "        #Preprocess the doc and form the vocab\n",
    "        vocab = set()\n",
    "        \n",
    "        file_path = count_to_file[doc]\n",
    "    \n",
    "        lines = \"\"\n",
    "        \n",
    "        f = open(file_path, 'r')\n",
    "        lines = f.read()\n",
    "        \n",
    "        words = tokenizer.tokenize(lines) #tokenize\n",
    "        words = [word.strip('_ ').lower() for word in words if word not in STOPWORDS] #lowercase \n",
    "        words = [stemmer.stem(word) for word in words] #stemmer\n",
    "        words = [word for word in words if word not in STOPWORDS and len(word) > 0] #stopword removal\n",
    "\n",
    "        for word in words:\n",
    "            vocab.add(word)\n",
    "            \n",
    "        #Find score and class\n",
    "        score = {}\n",
    "        \n",
    "        maxx = -float(\"inf\") #Max score for the doc\n",
    "        maxx_class = cls #Class with the max score\n",
    "        \n",
    "        for c in class_dic:\n",
    "            if c not in score:\n",
    "                score[c] = 0\n",
    "            \n",
    "            score[c] += prior[c]\n",
    "                \n",
    "            summ = 0\n",
    "            for t_ in freq_dic[c]:\n",
    "                summ += freq_dic[c][t_] \n",
    "            summ += total_vocab_size #Smoothing (Denominator)\n",
    "\n",
    "            for t in vocab:\n",
    "                if t in cond_prob[c]:\n",
    "                    score[c] += cond_prob[c][t]\n",
    "                else: #If the doc contains a term not in the training doc, we use smoothing to avoid -inf\n",
    "                    tmp = np.log(1/summ)*100\n",
    "                    score[c] += tmp\n",
    "                    \n",
    "            if (score[c] > maxx):\n",
    "                maxx = score[c]\n",
    "                maxx_class = c\n",
    "        \n",
    "        class_found.append(maxx_class)\n",
    "        class_true.append(cls)\n",
    "        \n",
    "        if (maxx_class == cls): #Predicted class is same as actual class\n",
    "            acc_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.13333333333334 %\n"
     ]
    }
   ],
   "source": [
    "# Accuracy\n",
    "print(\"Accuracy:\", (acc_count/N_test)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True vs Predicted\n",
      "[[ 273.    1.   17.    5.    4.]\n",
      " [   2.  296.    1.    1.    0.]\n",
      " [   6.    0.  285.    6.    3.]\n",
      " [   1.    0.    3.  296.    0.]\n",
      " [   0.    1.    3.    4.  292.]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrix\n",
    "\n",
    "mat = np.zeros((len(class_dic), len(class_dic)))\n",
    "\n",
    "for it in range(len(class_found)):\n",
    "    mat[int(class_true[it])][int(class_found[it])] += 1\n",
    "\n",
    "df = pd.DataFrame(mat)\n",
    "print(\"True vs Predicted\")\n",
    "print(mat)\n",
    "\n",
    "# Stylish Output\n",
    "# df = pd.DataFrame(mat)\n",
    "# cm = sns.light_palette(\"orange\", as_cmap=True)\n",
    "# df.style.background_gradient(cmap=cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Confusion Matirx from Pandas\n",
    "\n",
    "# from pandas_ml import ConfusionMatrix\n",
    "# df_confusion = pd.crosstab(pd.Series(class_true), pd.Series(class_found), rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "# df_confusion.style"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
