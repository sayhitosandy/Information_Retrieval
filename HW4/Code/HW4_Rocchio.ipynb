{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Import packages\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import json\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import STOPWORDS\n",
    "from random import shuffle\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Pre-processing\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+') #Tokenizer\n",
    "stemmer = SnowballStemmer('english') #Snowball Stemmer\n",
    "stops = set(stopwords.words('english')) #Stopwords\n",
    "for i in stops:\n",
    "    STOPWORDS.add(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd() #Current Working Directory\n",
    "folders_path = os.path.join(cwd, r\"Dataset\\20_newsgroups\")\n",
    "folders = os.listdir(folders_path) #List of folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count_to_file = {} #Dictionary that maps file no to file path\n",
    "file_to_count = {} #Dictionary that maps file path to file no\n",
    "count = 0\n",
    "\n",
    "for root, _, files in os.walk(folders_path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        count += 1\n",
    "        \n",
    "        file_to_count[file_path] = str(count)\n",
    "        count_to_file[str(count)] = file_path\n",
    "        \n",
    "fp = open(os.path.join(cwd, r'Dataset\\count_to_file.json'), 'w+')\n",
    "json.dump(count_to_file, fp, sort_keys=True)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\file_to_count.json'), 'w+')\n",
    "json.dump(file_to_count, fp, sort_keys=True)\n",
    "fp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = {} #Dictionary that stores list of training files for each class\n",
    "test_data = {} #Dictionary that stores list of test files for each class\n",
    "class_dic = {} #Dictionary that maps class no to class/folder path\n",
    "cls_count = 0\n",
    "ratio = 0.9 #Training Data split ratio\n",
    "\n",
    "for root, _, files in os.walk(folders_path):\n",
    "    if (len(files) == 0):\n",
    "        continue\n",
    "    \n",
    "    class_dic[str(cls_count)] = root\n",
    "    train_data_len = int(ratio * len(files))\n",
    "    test_data_len = len(files) - train_data_len\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        data.append(file_to_count[file_path])\n",
    "    \n",
    "    shuffle(data) #Randomly shuffle data to split into test and train\n",
    "    \n",
    "    training_data[str(cls_count)] = data[:train_data_len]\n",
    "    test_data[str(cls_count)] = data[-test_data_len:]\n",
    "    \n",
    "    cls_count += 1\n",
    "    \n",
    "fp = open(os.path.join(cwd, r'Dataset\\training_data.json'), 'w+')\n",
    "json.dump(training_data, fp, sort_keys=True)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\test_data.json'), 'w+')\n",
    "json.dump(test_data, fp, sort_keys=True)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\class_dic.json'), 'w+')\n",
    "json.dump(class_dic, fp, sort_keys=True)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16601\n",
      "18191\n",
      "18227\n",
      "14284\n",
      "17114\n"
     ]
    }
   ],
   "source": [
    "all_words = {} #Unique words in a class\n",
    "train_tfidf_dic = {} #Inverted index with tfs\n",
    "train_doc_vector_dic = {} #doc to term mapping with tf-idf (temp)\n",
    "\n",
    "for cls in class_dic:\n",
    "    if cls not in all_words:\n",
    "        all_words[cls] = set() #initialize\n",
    "        \n",
    "    files = training_data[cls]\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = count_to_file[file]\n",
    "        file_id = file_to_count[file_path]\n",
    "    \n",
    "        lines = \"\"\n",
    "        \n",
    "        f = open(file_path, 'r')\n",
    "\n",
    "        local_freq_dic = {} #store only the freq of words appearing in one file\n",
    "\n",
    "        lines = f.read()\n",
    "\n",
    "        words = tokenizer.tokenize(lines) #tokenize\n",
    "        words = [word.strip('_ ').lower() for word in words if word not in STOPWORDS] #lowercase \n",
    "        words = [stemmer.stem(word) for word in words] #stemmer\n",
    "        words = [word for word in words if word not in STOPWORDS and len(word) > 0] #stopword removal\n",
    "\n",
    "        for word in words:\n",
    "            all_words[cls].add(word)\n",
    "            \n",
    "        for word in words:\n",
    "            if word not in local_freq_dic:\n",
    "                local_freq_dic[word] = 0 #initialize\n",
    "            local_freq_dic[word] += 1\n",
    "\n",
    "        for word in local_freq_dic:\n",
    "            if word not in train_tfidf_dic:\n",
    "                train_tfidf_dic[word] = {} #initialize\n",
    "            train_tfidf_dic[word][file_id] = local_freq_dic[word]\n",
    "\n",
    "        if file_id not in train_doc_vector_dic:\n",
    "            train_doc_vector_dic[file_id] = {}\n",
    "            for word in local_freq_dic:\n",
    "                train_doc_vector_dic[file_id][word] = 0 #initialize\n",
    "             \n",
    "    all_words[cls] = list(all_words[cls])\n",
    "    print(len(all_words[cls]))\n",
    "    \n",
    "fp = open(os.path.join(cwd, r'Dataset\\all_words.json'), 'w+')\n",
    "json.dump(all_words, fp, sort_keys=True)\n",
    "fp.close()\n",
    "\n",
    "for word in train_tfidf_dic:\n",
    "    df = len(train_tfidf_dic[word].keys())\n",
    "    idf = (np.log10((train_data_len+1)/df))\n",
    "    for i in train_tfidf_dic[word]:\n",
    "        train_tfidf_dic[word][i] = ((1+np.log10(train_tfidf_dic[word][i]))*idf) #Formula for tf-idf\n",
    "        \n",
    "for doc_id in train_doc_vector_dic:\n",
    "    for word in train_doc_vector_dic[doc_id]:\n",
    "        train_doc_vector_dic[doc_id][word] = train_tfidf_dic[word][doc_id] #Create document vector for tf-idf vector space model\n",
    "        \n",
    "fp = open(os.path.join(cwd, r'Dataset\\train_tfidf_dic.json'), 'w+')\n",
    "json.dump(train_tfidf_dic, fp, sort_keys=True)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\train_doc_vector_dic.json'), 'w+')\n",
    "json.dump(train_doc_vector_dic, fp, sort_keys=True)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load dictionaries\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\count_to_file.json'), 'r')\n",
    "count_to_file = json.load(fp)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\file_to_count.json'), 'r')\n",
    "file_to_count = json.load(fp)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\training_data.json'), 'r')\n",
    "training_dataa = json.load(fp)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\test_data.json'), 'r')\n",
    "test_data = json.load(fp)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\class_dic.json'), 'r')\n",
    "class_dic = json.load(fp)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\train_tfidf_dic.json'), 'r')\n",
    "train_tfidf_dic = json.load(fp)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\train_doc_vector_dic.json'), 'r')\n",
    "train_doc_vector_dic = json.load(fp)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Rocchio Training\n",
    "\n",
    "centroid = {}\n",
    "\n",
    "for cls in class_dic:\n",
    "    if cls not in centroid:\n",
    "        centroid[cls] = {}\n",
    "        for word in all_words[cls]:\n",
    "            if word not in centroid[cls]:\n",
    "                centroid[cls][word] = 0\n",
    "    \n",
    "    files = training_data[cls]\n",
    "    \n",
    "    for file in files:\n",
    "        for word in all_words[cls]:\n",
    "            if word in train_doc_vector_dic[file]:\n",
    "                centroid[cls][word] += train_doc_vector_dic[file][word]\n",
    "    \n",
    "    for word in centroid[cls]:\n",
    "        centroid[cls][word] = (centroid[cls][word]/len(files))  \n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\centroid.json'), 'w+')\n",
    "json.dump(centroid, fp, sort_keys=True)\n",
    "fp.close()\n",
    "\n",
    "#     print(cls, centroid[cls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Rocchio Testing\n",
    "\n",
    "test_tfidf_dic = {} #Inverted index with tfs\n",
    "test_doc_vector_dic = {} #doc to term mapping with tf-idf (temp)\n",
    "\n",
    "for cls in class_dic:\n",
    "    files = test_data[cls]\n",
    "    \n",
    "    for file in files:\n",
    "        file_path = count_to_file[file]\n",
    "        file_id = file_to_count[file_path]\n",
    "    \n",
    "        lines = \"\"\n",
    "        \n",
    "        f = open(file_path, 'r')\n",
    "\n",
    "        local_freq_dic = {} #store only the freq of words appearing in one file\n",
    "\n",
    "        lines = f.read()\n",
    "\n",
    "        words = tokenizer.tokenize(lines) #tokenize\n",
    "        words = [word.strip('_ ').lower() for word in words if word not in STOPWORDS] #lowercase \n",
    "        words = [stemmer.stem(word) for word in words] #stemmer\n",
    "        words = [word for word in words if word not in STOPWORDS and len(word) > 0] #stopword removal\n",
    "\n",
    "        for word in words:\n",
    "            if word not in local_freq_dic:\n",
    "                local_freq_dic[word] = 0 #initialize\n",
    "            local_freq_dic[word] += 1\n",
    "\n",
    "        for word in local_freq_dic:\n",
    "            if word not in test_tfidf_dic:\n",
    "                test_tfidf_dic[word] = {} #initialize\n",
    "            test_tfidf_dic[word][file_id] = local_freq_dic[word]\n",
    "\n",
    "        if file_id not in test_doc_vector_dic:\n",
    "            test_doc_vector_dic[file_id] = {}\n",
    "            for word in local_freq_dic:\n",
    "                test_doc_vector_dic[file_id][word] = 0 #initialize\n",
    "             \n",
    "    \n",
    "for word in test_tfidf_dic:\n",
    "    if word in train_tfidf_dic:\n",
    "        df = len(train_tfidf_dic[word].keys()) + 1\n",
    "    else:\n",
    "        df = 1\n",
    "    idf = (np.log10((train_data_len+1)/df))\n",
    "    for i in test_tfidf_dic[word]:\n",
    "        test_tfidf_dic[word][i] = ((1+np.log10(test_tfidf_dic[word][i]))*idf) #Formula for tf-idf\n",
    "        \n",
    "for doc_id in test_doc_vector_dic:\n",
    "    for word in test_doc_vector_dic[doc_id]:\n",
    "        test_doc_vector_dic[doc_id][word] = test_tfidf_dic[word][doc_id] #Create document vector for tf-idf vector space model\n",
    "        \n",
    "fp = open(os.path.join(cwd, r'Dataset\\test_tfidf_dic.json'), 'w+')\n",
    "json.dump(test_tfidf_dic, fp, sort_keys=True)\n",
    "fp.close()\n",
    "\n",
    "fp = open(os.path.join(cwd, r'Dataset\\test_doc_vector_dic.json'), 'w+')\n",
    "json.dump(test_doc_vector_dic, fp, sort_keys=True)\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.8 %\n"
     ]
    }
   ],
   "source": [
    "acc_count = 0\n",
    "class_true = []\n",
    "class_found = []\n",
    "\n",
    "for doc in test_doc_vector_dic:\n",
    "    min_class = -1\n",
    "    min_valu = 1e9+7\n",
    "    \n",
    "    for cls in class_dic:\n",
    "        centroid_vec = centroid[cls]\n",
    "        doc_vec = test_doc_vector_dic[doc]\n",
    "        \n",
    "        words = set()\n",
    "        for word in centroid_vec:\n",
    "            words.add(word)\n",
    "        for word in doc_vec:\n",
    "            words.add(word)\n",
    "            \n",
    "        words = list(words)\n",
    "#         print(len(words))\n",
    "        \n",
    "        valu = 0\n",
    "        for word in words:\n",
    "            if word in centroid_vec and word in doc_vec:\n",
    "                valu += (centroid_vec[word] - doc_vec[word])**2\n",
    "            elif word in centroid_vec:\n",
    "                valu += (centroid_vec[word])**2\n",
    "            elif word in doc_vec:\n",
    "                valu += (doc_vec[word])**2\n",
    "        \n",
    "        valu = valu**0.5\n",
    "\n",
    "        #         print(cls, valu)\n",
    "        if valu < min_valu:\n",
    "            min_valu = valu\n",
    "            min_class = cls\n",
    "    \n",
    "    class_found.append(min_class)\n",
    "            \n",
    "    for file in test_data:\n",
    "        if doc in test_data[file]:\n",
    "            actual_class = file\n",
    "            class_true.append(actual_class)\n",
    "    \n",
    "#     print(actual_class, min_class)\n",
    "    if actual_class == min_class:\n",
    "        acc_count += 1\n",
    "        \n",
    "print(\"Accuracy:\", (acc_count/len(test_doc_vector_dic.keys()))*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True vs Predicted\n",
      "[[ 98.   0.   0.   1.   1.]\n",
      " [  7.  93.   0.   0.   0.]\n",
      " [ 12.   0.  88.   0.   0.]\n",
      " [  4.   0.   0.  96.   0.]\n",
      " [  5.   0.   0.   1.  94.]]\n"
     ]
    }
   ],
   "source": [
    "mat = np.zeros((len(class_dic), len(class_dic)))\n",
    "\n",
    "for it in range(len(class_found)):\n",
    "    mat[int(class_true[it])][int(class_found[it])] += 1\n",
    "\n",
    "df = pd.DataFrame(mat)\n",
    "print(\"True vs Predicted\")\n",
    "print(mat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.7\n",
      "0.8\n",
      "0.9\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
